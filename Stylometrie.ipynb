{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fab4cd",
   "metadata": {},
   "source": [
    "# Le but de ce notebook est d'identifier des caractéristiques permettant de différencier des textes écrits par Molière et Corneille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09dcc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import List,Set,Tuple,Dict\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install unidecode\n",
    "#!pip install wordcloud\n",
    "#!pip install pillow\n",
    "\n",
    "# le répertoire de travail\n",
    "directory = os.path.abspath('')\n",
    "random.seed(42)\n",
    "\n",
    "# retourne tous les fichiers *.txt présents dans le repertoire 'path'\n",
    "def all_txt_files_in_directory(path: str):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# infique si la ligne 'ligne' est une ligne valide ou si elle doit être ignorée (par exemple si elle vide)\n",
    "def is_valid_line(line: str) -> bool:\n",
    "    if line.startswith('Scène ') or line.startswith('Acte '):\n",
    "        return False\n",
    "    if len(line)<10:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split_book_into_paragraphs(path: str, min_paragraph_length) -> List[str]:\n",
    "    result = []\n",
    "    with open(path) as file:\n",
    "        current_paragraph = \"\"\n",
    "        for line in file:\n",
    "            if is_valid_line(line):\n",
    "                if current_paragraph :\n",
    "                    current_paragraph += \"\\n\"\n",
    "                current_paragraph += line.rstrip()\n",
    "                if len(current_paragraph) >= min_paragraph_length:\n",
    "                    result.append(current_paragraph) \n",
    "                    current_paragraph = \"\"\n",
    "    if len(current_paragraph) >= min_paragraph_length or (current_paragraph and len(result) == 0):\n",
    "        result.append(current_paragraph) \n",
    "    return result\n",
    "\n",
    "def load_all_books(path: str, min_paragraph_length:int) -> Dict[str,List[str]]:\n",
    "    book_to_paragraphs = dict()\n",
    "    for book_path in all_txt_files_in_directory(path):\n",
    "        book_to_paragraphs[pathlib.Path(book_path).stem] = split_book_into_paragraphs(book_path, min_paragraph_length)\n",
    "    return book_to_paragraphs\n",
    "\n",
    "\n",
    "# pour supprimer les accents\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def remove_diacritics(word: str) -> str:\n",
    "    import unidecode  \n",
    "    return unidecode.unidecode(word)\n",
    "\n",
    "# mots en minuscules\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def to_lowercase(word: str) -> str:\n",
    "    return word.lower()\n",
    "\n",
    "# pour le stemming\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def to_stemming(word: str) -> str:\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def normalize_word(word: str, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> str:\n",
    "    if use_lowercase:\n",
    "        word = to_lowercase(word)\n",
    "    if use_diacritics:\n",
    "        word = remove_diacritics(word)\n",
    "    if use_stemming:\n",
    "        word = to_stemming(word)\n",
    "    return word\n",
    "\n",
    "def paragraph_count(book_to_paragraphs: Dict[str, List[str]]) -> int:\n",
    "    if not book_to_paragraphs:\n",
    "        return 0\n",
    "    return sum([len(c) for c in book_to_paragraphs.values()])\n",
    "\n",
    "def split_text(text:str) -> List[str]:\n",
    "    return re.findall(r\"\\b[\\w'^\\d]+\\b\", text.rstrip())\n",
    "            \n",
    "def word_count(book_to_paragraphs: Dict[str, List[str]]) -> int:\n",
    "    result = 0\n",
    "    for paragraph in all_paragraphs(book_to_paragraphs):\n",
    "        result += len(split_text(paragraph.rstrip()))\n",
    "    return result\n",
    "\n",
    "def all_paragraphs(book_to_paragraphs: Dict[str, List[str]]) -> List[str]:\n",
    "    result = []\n",
    "    for p in book_to_paragraphs.values():\n",
    "        result.extend(p)\n",
    "    return result\n",
    "\n",
    "# reduce the dataset so that it contains exactly 'target_count' paragraphs\n",
    "def reduce_to_paragraph_count(book_to_paragraphs: dict, target_count: int ) -> int:\n",
    "    current_count = paragraph_count(book_to_paragraphs)\n",
    "    if current_count<target_count:\n",
    "        raise Exception(f'current_count {current_count} < target_count {target_count}')\n",
    "    to_remove = current_count-  target_count\n",
    "    result = dict()\n",
    "    for book, paragraphs in sorted(book_to_paragraphs.items(), key =lambda x : len(x[1])):\n",
    "        if len(paragraphs)<=to_remove:\n",
    "            to_remove-=len(paragraphs)\n",
    "            continue\n",
    "        result[book] = paragraphs[:len(paragraphs)-to_remove]\n",
    "        to_remove = 0\n",
    "    return result\n",
    "\n",
    "def compute_most_common_normalized_words(normalized_words_to_stats: dict, count: int) -> Dict[str,float]:\n",
    "    sorted_by_total_count = sorted(normalized_words_to_stats.items(), key=lambda item:item[1][0], reverse=True)[:count]\n",
    "    total_count = sum([c[1][0] for c in sorted_by_total_count])\n",
    "    result  = dict()\n",
    "    for normalied_word,stats in sorted_by_total_count:\n",
    "        result[normalied_word] = stats[0]/total_count\n",
    "    return result\n",
    "\n",
    "def compute_normalized_words_to_stats(paragraphs: List[str], hyperparameters: dict) -> Dict[str, Dict[int,Dict[str,int]] ]:\n",
    "    use_lowercase = hyperparameters['use_lowercase']\n",
    "    use_diacritics = hyperparameters['use_diacritics']\n",
    "    use_stemming = hyperparameters['use_stemming']\n",
    "    normalized_words_to_stats = dict()\n",
    "    for paragraph in paragraphs:\n",
    "        words = re.findall(r\"\\b[\\w'^\\d]+\\b\", paragraph.rstrip())\n",
    "        for original_word in words:\n",
    "            normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "            if normalized_word not in normalized_words_to_stats:\n",
    "                normalized_words_to_stats[normalized_word] = (0, dict())\n",
    "            count,original_word_count = normalized_words_to_stats[normalized_word]\n",
    "            if original_word not in original_word_count:\n",
    "                original_word_count[original_word] = 1\n",
    "            else:\n",
    "                original_word_count[original_word] += 1\n",
    "            normalized_words_to_stats[normalized_word] = (count+1,original_word_count)\n",
    "    return normalized_words_to_stats\n",
    "\n",
    "def get_max_item(dic: Dict[str, int]) -> Tuple[str,int]:\n",
    "    key_max_count, max_count = (None, None)\n",
    "    for key,count in dic.items():\n",
    "        if max_count is None or count > max_count:\n",
    "            key_max_count, max_count = key, count\n",
    "    return key_max_count, max_count\n",
    "    \n",
    "def split_train_validation_single_author(book_to_paragraphs: dict, percentage_in_train:float) :\n",
    "    books = list(book_to_paragraphs.keys())\n",
    "    train = dict()\n",
    "    validation = dict()\n",
    "    for book, paragraphs in book_to_paragraphs.items():\n",
    "        paragraphs_count = len(paragraphs)\n",
    "        percentage_in_train_if_adding_to_train = (paragraph_count(train)+len(paragraphs))/max(paragraph_count(train)+paragraph_count(validation)+len(paragraphs),1)\n",
    "        percentage_in_train_if_adding_to_validation = paragraph_count(train)/max(paragraph_count(train)+paragraph_count(validation)+len(paragraphs),1)\n",
    "        if abs(percentage_in_train_if_adding_to_train-percentage_in_train)<abs(percentage_in_train_if_adding_to_validation-percentage_in_train):\n",
    "            train[book] = paragraphs\n",
    "        else:\n",
    "            validation[book] = paragraphs\n",
    "    return train, validation\n",
    "\n",
    "def split_train_validation_all_authors(book_to_paragraphs_author1: dict, book_to_paragraphs_author2: dict, percentage_in_train:float) :\n",
    "    train_author1,validation_author1 = split_train_validation_single_author(book_to_paragraphs_author1, percentage_in_train)\n",
    "    train_author2,validation_author2 = split_train_validation_single_author(book_to_paragraphs_author2, percentage_in_train)\n",
    "\n",
    "    target_length_validation = min(paragraph_count(validation_author1),paragraph_count(validation_author2))            \n",
    "    validation_author1 = reduce_to_paragraph_count(validation_author1, target_length_validation)\n",
    "    validation_author2 = reduce_to_paragraph_count(validation_author2, target_length_validation)\n",
    "\n",
    "    target_length_train = min(paragraph_count(train_author1),paragraph_count(train_author2))            \n",
    "    train_author1 = reduce_to_paragraph_count(train_author1, target_length_train)\n",
    "    train_author2 = reduce_to_paragraph_count(train_author2, target_length_train)\n",
    "\n",
    "    proportion_in_train = target_length_train/(target_length_train+target_length_validation)\n",
    "    if proportion_in_train>percentage_in_train:\n",
    "        target_length_train = int( (percentage_in_train/(1-percentage_in_train)) *target_length_validation )\n",
    "        train_author1 = reduce_to_paragraph_count(train_author1, target_length_train)\n",
    "        train_author2 = reduce_to_paragraph_count(train_author2, target_length_train)\n",
    "    else:\n",
    "        target_length_validation = int( ((1-percentage_in_train)/percentage_in_train) *target_length_train )\n",
    "        validation_author1 = reduce_to_paragraph_count(validation_author1, target_length_validation)\n",
    "        validation_author2 = reduce_to_paragraph_count(validation_author2, target_length_validation)\n",
    "    return train_author1,validation_author1,train_author2,validation_author2\n",
    "\n",
    "def calcul_accuracy(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (TP+TN)/max(TP+TN+FP+FN,1)\n",
    "\n",
    "def calcul_accuracy_corneille(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TN/max(TN+FP,1)\n",
    "\n",
    "def calcul_accuracy_moliere(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TP/max(TP+FN,1)\n",
    "\n",
    "def calcul_accuracy(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (TP+TN)/max(TP+TN+FP+FN,1)\n",
    "\n",
    "def compute_author_score_v1(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "def compute_author_score_v2(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1+most_common_words_for_author[normalized_word]\n",
    "    return score\n",
    "    \n",
    "def compute_author_score_single_author(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    count = 0\n",
    "    splitted_text = split_text(text)\n",
    "    for original_word in splitted_text:\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            count += 1\n",
    "    return count/len(splitted_text)\n",
    "\n",
    "def compute_author_score_v2(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1+most_common_words_for_author[normalized_word]\n",
    "    return score\n",
    "\n",
    "def compute_confusion_matrix_single_author(author_name:str, texts_from_author: List[str], text_from_other_authors: List[str], most_common_words_from_author: dict , hyperparameters: dict, verbose:bool) ->Tuple[int,int,int,int]:\n",
    "    TP = 0 # y_true = author_name ,  y_pred = author_name\n",
    "    TN = 0 # y_true = another author, y_pred = another author\n",
    "    FN = 0 # y_true = author_name,   y_pred = another author\n",
    "    FP = 0 # y_true = another_author, y_pred = author_name \n",
    "    threshold_author_score = hyperparameters['threshold_author_score']\n",
    "    use_lowercase = hyperparameters['use_lowercase']\n",
    "    use_diacritics = hyperparameters['use_diacritics']\n",
    "    use_stemming = hyperparameters['use_stemming']\n",
    "    for t in texts_from_author:\n",
    "        score_author = compute_author_score_v1(t, most_common_words_from_author, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author>=threshold_author_score:\n",
    "            if TP == 0 and verbose:\n",
    "                print(f'\\nExemple de TP (Texte de {author_name}, bien identifié, score {author_name}: {round(score_author,4)}):\\n{t}\\n')\n",
    "            TP += 1\n",
    "        else:\n",
    "            if FN == 0 and verbose:\n",
    "                print(f'\\nExemple de FN (Texte de {author_name}, mal identifié, score {author_name}: {round(score_author,4)}):\\n{t}\\n')\n",
    "            FN += 1\n",
    "    for t in text_from_other_authors:\n",
    "        score_author = compute_author_score_v1(t, most_common_words_from_author, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author>=threshold_author_score:\n",
    "            if FP == 0 and verbose:\n",
    "                print(f\"\\nExemple de FP (Texte d'un autre auteur, mal identifié, score {score_author}: {round(score_author,4)}):\\n{t}\\n\")\n",
    "            FP += 1\n",
    "        else:\n",
    "            if TN == 0 and verbose:\n",
    "                print(f\"\\nExemple de TN (Texte d'un autre auteur, bien identifié, score {score_author}: {round(score_author,4)}):\\n{t}\\n\")\n",
    "            TN += 1\n",
    "    return (TP,TN,FP,FN)\n",
    "        \n",
    "def train_single_author(hyperparameters: dict, author_directory: str, author_name, other_authors_directory: str,verbose: bool):\n",
    "    random.seed(42)\n",
    "    min_paragraph_length = hyperparameters['min_paragraph_length']\n",
    "    author_dataset = load_all_books(author_directory, min_paragraph_length)\n",
    "    other_authors_dataset = load_all_books(other_authors_directory, min_paragraph_length)\n",
    "    if verbose: \n",
    "        print(f'\\n{author_name} Dataset: {paragraph_count(author_dataset)} paragraphes ({word_count(author_dataset)} mots) venant de {len(author_dataset)} oeuvres:\\n{list(author_dataset.keys())}')\n",
    "        print(f'\\nother_authors Dataset: {paragraph_count(other_authors_dataset)} paragraphes ({word_count(other_authors_dataset)} mots) venant de {len(other_authors_dataset)} oeuvres:\\n{list(other_authors_dataset.keys())}')\n",
    "\n",
    "    percentage_in_train = hyperparameters['percentage_in_train']\n",
    "    train_author,validation_author,train_other_authors,validation_other_authors = split_train_validation_all_authors(author_dataset, other_authors_dataset, percentage_in_train)\n",
    "\n",
    "    if verbose: \n",
    "        print(f'\\n{author_name} Train Dataset: {paragraph_count(train_author)} paragraphes ({word_count(train_author)} mots) venant de {len(train_author)} oeuvres:\\n{list(train_author.keys())}')\n",
    "        print(f'\\n{author_name} Validation Dataset: {paragraph_count(validation_author)} paragraphes ({word_count(validation_author)} mots) venant de {len(validation_author)} oeuvres:\\n{list(validation_author.keys())}')\n",
    "        print(f'\\nother_authors Train Dataset: {paragraph_count(train_other_authors)} paragraphes ({word_count(train_other_authors)} mots) venant de {len(train_other_authors)} oeuvres:\\n{list(train_other_authors.keys())}')\n",
    "        print(f'\\nother_authors Validation Dataset: {paragraph_count(validation_other_authors)} paragraphes ({word_count(validation_other_authors)} mots) venant de {len(validation_other_authors)} oeuvres:\\n{list(validation_other_authors.keys())}')\n",
    "\n",
    "    normalized_words_to_stats_author = compute_normalized_words_to_stats(all_paragraphs(train_author), hyperparameters)\n",
    "    # we only keep the most common words\n",
    "    most_common_author = compute_most_common_normalized_words(normalized_words_to_stats_author, hyperparameters['most_common_normalized_words_count'])\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix_single_author(author_name,all_paragraphs(validation_author), all_paragraphs(validation_other_authors), most_common_author , hyperparameters, verbose)\n",
    "    return (TP,TN,FP,FN,train_author,validation_author, validation_other_authors, normalized_words_to_stats_author, most_common_author)\n",
    "\n",
    "def display_wordcloud(most_common_normalized_words: dict, normalized_words_to_stats: dict, title:str) -> None:\n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "    word_frequencies = dict()\n",
    "    for normalized_word, frequency in most_common_normalized_words.items():\n",
    "        original_word = get_max_item(normalized_words_to_stats[normalized_word][1])[0]\n",
    "        if len(original_word) > 4:\n",
    "            word_frequencies[original_word] = frequency\n",
    "    wordcloud = WordCloud(width=1200, height=600, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "    # Display the generated word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3baff9",
   "metadata": {},
   "source": [
    "## Création d'un fichier de statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd281715",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparameters = dict()\n",
    "hyperparameters['use_lowercase'] = True\n",
    "hyperparameters['use_diacritics'] = True\n",
    "hyperparameters['use_stemming'] = True\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "\n",
    "moliere_dataset = load_all_books(os.path.join(directory, 'moliere'), hyperparameters['min_paragraph_length'])\n",
    "stats_moliere = compute_normalized_words_to_stats(all_paragraphs(moliere_dataset), hyperparameters)\n",
    "corneille_dataset = load_all_books(os.path.join(directory, 'corneille'), hyperparameters['min_paragraph_length'])\n",
    "stats_corneille = compute_normalized_words_to_stats(all_paragraphs(corneille_dataset), hyperparameters)\n",
    "\n",
    "moliere_total_word_count = sum([c[0] for c in stats_moliere.values()])\n",
    "corneille_total_word_count = sum([c[0] for c in stats_corneille.values()])\n",
    "\n",
    "normalized_words = list((set(stats_moliere.keys())|set(stats_corneille.keys())))\n",
    "normalized_words.sort()\n",
    "moliere_normalized_word_count_in_percentage = []\n",
    "moliere_normalized_word_count = []\n",
    "corneille_normalized_word_count_in_percentage = []\n",
    "corneille_normalized_word_count = []\n",
    "moliere_most_common_original_word = []\n",
    "moliere_most_common_original_word_count = []\n",
    "corneille_most_common_original_word = []\n",
    "corneille_most_common_original_word_count = []\n",
    "\n",
    "\n",
    "for normalized_word in normalized_words:\n",
    "    if normalized_word in stats_moliere:\n",
    "        stat = stats_moliere[normalized_word]\n",
    "        moliere_normalized_word_count_in_percentage.append(stat[0]/moliere_total_word_count)\n",
    "        moliere_normalized_word_count.append(stat[0])\n",
    "        most_common_word, most_common_word_count = get_max_item(stat[1])\n",
    "        moliere_most_common_original_word.append(most_common_word)\n",
    "        moliere_most_common_original_word_count.append(most_common_word_count)\n",
    "    else:\n",
    "        moliere_normalized_word_count_in_percentage.append(0)\n",
    "        moliere_normalized_word_count.append(0)\n",
    "        moliere_most_common_original_word.append(None)\n",
    "        moliere_most_common_original_word_count.append(None)\n",
    "    if normalized_word in stats_corneille:\n",
    "        stat = stats_corneille[normalized_word]\n",
    "        corneille_normalized_word_count_in_percentage.append(stat[0]/corneille_total_word_count)\n",
    "        corneille_normalized_word_count.append(stat[0])\n",
    "        most_common_word, most_common_word_count = get_max_item(stat[1])\n",
    "        corneille_most_common_original_word.append(most_common_word)\n",
    "        corneille_most_common_original_word_count.append(most_common_word_count)\n",
    "    else:\n",
    "        corneille_normalized_word_count_in_percentage.append(0)\n",
    "        corneille_normalized_word_count.append(0)\n",
    "        corneille_most_common_original_word.append(None)\n",
    "        corneille_most_common_original_word_count.append(None)\n",
    "\n",
    "fhr_stats = pd.DataFrame(\n",
    "    {'normalized_words': normalized_words,\n",
    "    'moliere_count_in_percentage': moliere_normalized_word_count_in_percentage,\n",
    "    'moliere_count': moliere_normalized_word_count,\n",
    "    'corneille_count_in_percentage' : corneille_normalized_word_count_in_percentage,\n",
    "    'corneille_count' : corneille_normalized_word_count,\n",
    "    'moliere_most_common_original_word' : moliere_most_common_original_word,\n",
    "    'moliere_most_common_original_word_count' : moliere_most_common_original_word_count,\n",
    "    'corneille_most_common_original_word' : corneille_most_common_original_word,\n",
    "    'corneille_most_common_original_word_count' : corneille_most_common_original_word_count,\n",
    "    })\n",
    "\n",
    "# on sauvegarde ces stats sur le disque\n",
    "fhr_stats.to_csv(os.path.join(directory, 'stylometrie_stats.csv'), index=False)         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782958cb",
   "metadata": {},
   "source": [
    "# Identification des textes de Molière"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6e47c",
   "metadata": {},
   "source": [
    "## Mots les plus communs chez Molière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9ee72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_common_moliere = compute_most_common_normalized_words(stats_moliere, 1000)\n",
    "display_wordcloud(most_common_moliere, stats_moliere, \"Molière (uniquement les mots d'au moins 5 lettres)\")\n",
    "print(\"Mots (d'au moins 5 lettres) les plus fréquents chez Molière\\n\", \"\\n\".join([str(c) for c in most_common_moliere.items() if len(c[0])>4][:20]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325a291",
   "metadata": {},
   "source": [
    "## Exemple d'analyse pour un texte de Molière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fef9f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texte_moliere = moliere_dataset['les_fourberies_de_scapin'][1]\n",
    "print('Texte à analyser\\n\\n',texte_moliere)\n",
    "#split_text(texte_moliere)\n",
    "\n",
    "most_common_moliere = compute_most_common_normalized_words(stats_moliere, hyperparameters['most_common_normalized_words_count'])\n",
    "\n",
    "most_common_in_text = dict()\n",
    "normalized_word_found_in_text = set()\n",
    "splitted_text = split_text(texte_moliere)\n",
    "normalized_word_found_in_text_count = 0\n",
    "for original_word in splitted_text:\n",
    "    normalized_word = normalize_word(original_word, hyperparameters['use_lowercase'], hyperparameters['use_diacritics'], hyperparameters['use_stemming'])\n",
    "    if normalized_word not in most_common_moliere:\n",
    "        continue\n",
    "    normalized_word_found_in_text_count += 1\n",
    "    if normalized_word in normalized_word_found_in_text:\n",
    "        continue # word already processed\n",
    "    normalized_word_found_in_text.add(normalized_word)\n",
    "    most_common_in_text[original_word] = most_common_moliere[normalized_word]\n",
    "\n",
    "most_common_in_text = sorted(most_common_in_text.items(), key = lambda x:x[1], reverse=True)       \n",
    "\n",
    "\n",
    "#most_common_in_text = most_common_in_text[:100]\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "keys = [c[0] for c in most_common_in_text]\n",
    "values = [c[1] for c in most_common_in_text]\n",
    "\n",
    "plt.bar(keys, values, color='blue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(f' {normalized_word_found_in_text_count} mots de ce texte (sur {len(splitted_text)}) font partie des mots les plus courants chez Molière', fontsize=15)\n",
    "plt.xlabel('Mot', fontsize=15)\n",
    "plt.ylabel('Fréquence du mot chez Molière', fontsize=15)\n",
    "plt.xticks(rotation=45, fontsize=11)\n",
    "plt.xlim(-0.5, len(keys) - 0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40b7e1",
   "metadata": {},
   "source": [
    "## Précision avec des paramètres par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ed5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_directory = os.path.join(directory, 'moliere')\n",
    "author_name = \"Molière\"\n",
    "other_authors_directory = os.path.join(directory, 'corneille')\n",
    "\n",
    "verbose = False\n",
    "\n",
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['percentage_in_train'] = 0.9\n",
    "hyperparameters['use_lowercase'] = False\n",
    "hyperparameters['use_diacritics'] = False\n",
    "hyperparameters['use_stemming'] = False\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "\n",
    "#la caractéristique à améliorer\n",
    "hyperparameters['threshold_author_score'] = 130\n",
    "\n",
    "\n",
    "(TP,TN,FP,FN,train_author,validation_author,validation_other_authors, normalized_words_to_stats_moliere, most_common_author) = train_single_author(hyperparameters, author_directory, author_name, other_authors_directory, verbose)\n",
    "print()\n",
    "print('-'*80)\n",
    "print(f'Précision({author_name}): {round(calcul_accuracy(TP,TN,FP,FN),4)} ')\n",
    "print('-'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd132d17",
   "metadata": {},
   "source": [
    "# Grid search pour améliorer cette précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0f7ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['percentage_in_train'] = 0.9\n",
    "hyperparameters['use_lowercase'] = False\n",
    "hyperparameters['use_diacritics'] = True\n",
    "hyperparameters['use_stemming'] = False\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "\n",
    "# la caractéristique à améliorer\n",
    "hyperparameters['threshold_author_score'] = 0\n",
    "\n",
    "best_score = None\n",
    "for most_common_normalized_words_count in [ hyperparameters['most_common_normalized_words_count'] ]:\n",
    "    hyperparameters['most_common_normalized_words_count'] = most_common_normalized_words_count\n",
    "    (TP,TN,FP,FN,train_author,validation_author,validation_other_authors, normalized_words_to_stats_moliere, most_common_author) = train_single_author(hyperparameters, author_directory, author_name, other_authors_directory, verbose)\n",
    "    for threshold_author_score in range(100,200,1):\n",
    "        hyperparameters['threshold_author_score'] = threshold_author_score\n",
    "        (TP,TN,FP,FN) = compute_confusion_matrix_single_author(author_name,all_paragraphs(validation_author), all_paragraphs(validation_other_authors), most_common_author , hyperparameters, verbose)\n",
    "        accuracy = calcul_accuracy(TP,TN,FP,FN)\n",
    "        if accuracy>0.0 and (best_score is None or accuracy>best_score):\n",
    "            best_score = accuracy\n",
    "            print(f\"threshold_author_score={hyperparameters['threshold_author_score']} => Précision({author_name})={round(accuracy,4)}\")            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563c234",
   "metadata": {},
   "source": [
    "# Identification des textes de Corneille"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797843f1",
   "metadata": {},
   "source": [
    "## Mots les plus communs chez Corneille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da97c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_common_corneille = compute_most_common_normalized_words(stats_corneille, 1000)\n",
    "display_wordcloud(most_common_corneille, stats_corneille, \"Corneille (uniquement les mots d'au moins 5 lettres)\")\n",
    "print(\"Mots (d'au moins 5 lettres) les plus fréquents chez Corneille\\n\", \"\\n\".join([str(c) for c in most_common_corneille.items() if len(c[0])>5][:20]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a085a69",
   "metadata": {},
   "source": [
    "## Précision avec des paramètres par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1987e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_directory = os.path.join(directory, 'corneille')\n",
    "author_name = \"Corneille\"\n",
    "other_authors_directory = os.path.join(directory, 'moliere')\n",
    "\n",
    "verbose = False\n",
    "\n",
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['percentage_in_train'] = 0.9\n",
    "hyperparameters['use_lowercase'] = False\n",
    "hyperparameters['use_diacritics'] = False\n",
    "hyperparameters['use_stemming'] = True\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "\n",
    "#la caractéristique à améliorer\n",
    "hyperparameters['threshold_author_score'] = 130\n",
    "\n",
    "\n",
    "(TP,TN,FP,FN,train_author,validation_author,validation_other_authors, normalized_words_to_stats_moliere, most_common_author) = train_single_author(hyperparameters, author_directory, author_name, other_authors_directory, verbose)\n",
    "print()\n",
    "print('-'*80)\n",
    "print(f'Précision({author_name}): {round(calcul_accuracy(TP,TN,FP,FN),4)} ')\n",
    "print('-'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9d1c8",
   "metadata": {},
   "source": [
    "# Grid search pour améliorer cette précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79366a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['percentage_in_train'] = 0.9\n",
    "hyperparameters['use_lowercase'] = False\n",
    "hyperparameters['use_diacritics'] = False\n",
    "hyperparameters['use_stemming'] = True\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "\n",
    "# la caractéristique à améliorer\n",
    "hyperparameters['threshold_author_score'] = 0\n",
    "\n",
    "author_directory = os.path.join(directory, 'corneille')\n",
    "author_name = \"Corneille\"\n",
    "other_authors_directory = os.path.join(directory, 'moliere')\n",
    "best_score = None\n",
    "for most_common_normalized_words_count in [ hyperparameters['most_common_normalized_words_count'] ]:\n",
    "    hyperparameters['most_common_normalized_words_count'] = most_common_normalized_words_count\n",
    "    (TP,TN,FP,FN,train_author,validation_author,validation_other_authors, normalized_words_to_stats_moliere, most_common_author) = train_single_author(hyperparameters, author_directory, author_name, other_authors_directory, verbose)\n",
    "    for threshold_author_score in range(100,200,1):\n",
    "        hyperparameters['threshold_author_score'] = threshold_author_score\n",
    "        (TP,TN,FP,FN) = compute_confusion_matrix_single_author(author_name,all_paragraphs(validation_author), all_paragraphs(validation_other_authors), most_common_author , hyperparameters, verbose)\n",
    "        accuracy = calcul_accuracy(TP,TN,FP,FN)\n",
    "        if accuracy>0.0 and (best_score is None or accuracy>best_score):\n",
    "            best_score = accuracy\n",
    "            print(f\"threshold_author_score={hyperparameters['threshold_author_score']} => Précision({auth})={round(accuracy,4)}\")            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5078a11",
   "metadata": {},
   "source": [
    "## Entraînement et calcul des métriques avec deux auteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c3a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_author_score_v1(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "def compute_author_score_v2(text:str, most_common_words_for_author: Dict[str,float], use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1+most_common_words_for_author[normalized_word]\n",
    "    return score\n",
    "    \n",
    "def compute_confusion_matrix_all_authors(text_author1: List[str], text_author2: List[str], most_common_words_author1: dict , most_common_words_author2: dict, hyperparameters: dict, verbose:bool) ->Tuple[int,int,int,int]:\n",
    "    TP = 0 # y_true = Molière ,  y_pred = Molière\n",
    "    TN = 0 # y_true = Corneille, y_pred = Corneille\n",
    "    FN = 0 # y_true = Molière,   y_pred = Corneille\n",
    "    FP = 0 # y_true = Corneille, y_pred = Molière \n",
    "    compute_author_score = hyperparameters['compute_author_score']\n",
    "    use_lowercase = hyperparameters['use_lowercase']\n",
    "    use_diacritics = hyperparameters['use_diacritics']\n",
    "    use_stemming = hyperparameters['use_stemming']\n",
    "    for t in text_author1:\n",
    "        score_author1 = compute_author_score(t, most_common_words_author1, use_lowercase, use_diacritics, use_stemming)\n",
    "        score_author2 = compute_author_score(t, most_common_words_author2, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author1>score_author2:\n",
    "            if TP == 0 and verbose:\n",
    "                print(f'\\nExemple de TP (Texte de Molière, bien identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            TP += 1\n",
    "        else:\n",
    "            if FN == 0 and verbose:\n",
    "                print(f'\\nExemple de FN (Texte de Molière, mal identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            FN += 1\n",
    "    for t in text_author2:\n",
    "        score_author1 = compute_author_score(t, most_common_words_author1, use_lowercase, use_diacritics, use_stemming)\n",
    "        score_author2 = compute_author_score(t, most_common_words_author2, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author1>score_author2:\n",
    "            if FP == 0 and verbose:\n",
    "                print(f'\\nExemple de FP (Texte de Corneille, mal identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            FP += 1\n",
    "        else:\n",
    "            if TN == 0 and verbose:\n",
    "                print(f'\\nExemple de TN (Texte de Corneille, bien identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            TN += 1\n",
    "    return (TP,TN,FP,FN)\n",
    "        \n",
    "    \n",
    "def train_all_authors(hyperparameters: dict, verbose: bool):\n",
    "    random.seed(42)\n",
    "    min_paragraph_length = hyperparameters['min_paragraph_length']\n",
    "    moliere_dataset = load_all_books(os.path.join(directory, 'moliere'), min_paragraph_length)\n",
    "    corneille_dataset = load_all_books(os.path.join(directory, 'corneille'), min_paragraph_length)\n",
    "    if verbose: \n",
    "        print(f'\\nMoliere Dataset: {paragraph_count(moliere_dataset)} paragraphes ({word_count(moliere_dataset)} mots) venant de {len(moliere_dataset)} oeuvres:\\n{list(moliere_dataset.keys())}')\n",
    "        print(f'\\nCorneille Dataset: {paragraph_count(corneille_dataset)} paragraphes ({word_count(corneille_dataset)} mots) venant de {len(corneille_dataset)} oeuvres:\\n{list(corneille_dataset.keys())}')\n",
    "\n",
    "    percentage_in_train = hyperparameters['percentage_in_train']\n",
    "    train_moliere,validation_moliere,train_corneille,validation_corneille = split_train_validation_all_authors(moliere_dataset, corneille_dataset, percentage_in_train)\n",
    "\n",
    "    if verbose: \n",
    "        print(f'\\nMoliere Train Dataset: {paragraph_count(train_moliere)} paragraphes ({word_count(train_moliere)} mots) venant de {len(train_moliere)} oeuvres:\\n{list(train_moliere.keys())}')\n",
    "        print(f'\\nMoliere Validation Dataset: {paragraph_count(validation_moliere)} paragraphes ({word_count(validation_moliere)} mots) venant de {len(validation_moliere)} oeuvres:\\n{list(validation_moliere.keys())}')\n",
    "        print(f'\\nCorneille Train Dataset: {paragraph_count(train_corneille)} paragraphes ({word_count(train_corneille)} mots) venant de {len(train_corneille)} oeuvres:\\n{list(train_corneille.keys())}')\n",
    "        print(f'\\nCorneille Validation Dataset: {paragraph_count(validation_corneille)} paragraphes ({word_count(validation_corneille)} mots) venant de {len(validation_corneille)} oeuvres:\\n{list(validation_corneille.keys())}')\n",
    "\n",
    "    normalized_words_to_stats_moliere = compute_normalized_words_to_stats(all_paragraphs(train_moliere), hyperparameters)\n",
    "    normalized_words_to_stats_corneille = compute_normalized_words_to_stats(all_paragraphs(train_corneille), hyperparameters)\n",
    "\n",
    "    # we only keep the most common words\n",
    "    most_common_moliere = compute_most_common_normalized_words(normalized_words_to_stats_moliere, hyperparameters['most_common_normalized_words_count'])\n",
    "    most_common_corneille = compute_most_common_normalized_words(normalized_words_to_stats_corneille, hyperparameters['most_common_normalized_words_count'])\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix_all_authors(all_paragraphs(validation_moliere), all_paragraphs(validation_corneille), most_common_moliere , most_common_corneille, hyperparameters, verbose)\n",
    "    return (TP,TN,FP,FN,train_moliere,validation_moliere,train_corneille,validation_corneille,normalized_words_to_stats_moliere, normalized_words_to_stats_corneille, most_common_moliere, most_common_corneille)\n",
    "\n",
    "verbose = False\n",
    "\n",
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 1000\n",
    "hyperparameters['percentage_in_train'] = 0.9\n",
    "hyperparameters['most_common_normalized_words_count'] = 1000\n",
    "hyperparameters['use_lowercase'] = False\n",
    "hyperparameters['use_diacritics'] = False\n",
    "hyperparameters['use_stemming'] = False\n",
    "hyperparameters['compute_author_score'] = compute_author_score_v1\n",
    "\n",
    "#if verbose:    print(hyperparameters)\n",
    "(TP,TN,FP,FN,train_moliere,validation_moliere,train_corneille,validation_corneille,normalized_words_to_stats_moliere, normalized_words_to_stats_corneille, most_common_moliere, most_common_corneille) = train_all_authors(hyperparameters, verbose)\n",
    "print()\n",
    "print('-'*80)\n",
    "print(f'Précision(Molière ou Corneille?)={round(calcul_accuracy(TP,TN,FP,FN),4)}')\n",
    "print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dde4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for normalized_word in ['monsieur', 'madam', 'plus', 'esprit', 'encor']:\n",
    "    print(f\"Le mot '{normalized_word}':\")\n",
    "    print(f'\\test présent {stats_moliere[normalized_word][0]} fois chez Molière:  ', stats_moliere[normalized_word][1])\n",
    "    print(f'\\test présent {stats_corneille[normalized_word][0]} fois chez Corneille:', stats_corneille[normalized_word][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721f0fd",
   "metadata": {},
   "source": [
    "## Précision pour chaque oeuvre utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aa076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('-'*80+'\\nPrécision pour chaque oeuvre de Moliere\\n'+'-'*80)\n",
    "accuracy_moliere = dict()\n",
    "for book_path in all_txt_files_in_directory(os.path.join(directory, 'moliere')):\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix_all_authors(split_book_into_paragraphs(book_path, hyperparameters['min_paragraph_length']), [], most_common_moliere , most_common_corneille, hyperparameters, False)\n",
    "    #print(f\"Accuracy '{pathlib.Path(book_path).stem}': {round(calcul_accuracy(TP,TN,FP,FN),4)}\")\n",
    "    accuracy_moliere[pathlib.Path(book_path).stem] = calcul_accuracy(TP,TN,FP,FN)\n",
    "for e in sorted(accuracy_moliere.items(), key=lambda x: x[1]):\n",
    "    print(e)\n",
    "\n",
    "print()\n",
    "print('-'*80+'\\nPrécision pour chaque oeuvre de Corneille\\n'+'-'*80)\n",
    "accuracy_corneille = dict()\n",
    "for book_path in all_txt_files_in_directory(os.path.join(directory, 'corneille')):\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix_all_authors([], split_book_into_paragraphs(book_path, hyperparameters['min_paragraph_length']), most_common_moliere , most_common_corneille, hyperparameters, False)\n",
    "    accuracy_corneille[pathlib.Path(book_path).stem] = calcul_accuracy(TP,TN,FP,FN)\n",
    "for e in sorted(accuracy_corneille.items(), key=lambda x: x[1]):\n",
    "    print(e)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
