{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fab4cd",
   "metadata": {},
   "source": [
    "# Le but de ce notebook est d'identifier des caractéristiques très simples permettant de différencier des textes écrits par plusieurs auteurs\n",
    "## (en l'occurrence Molière et Corneille)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4b8ee",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import functools\n",
    "from typing import List,Set,Tuple,Dict\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install unidecode\n",
    "#!pip install wordcloud\n",
    "#!pip install pillow\n",
    "\n",
    "# le répertoire de travail\n",
    "directory = os.path.abspath('')\n",
    "random.seed(42)\n",
    "\n",
    "# retourne tous les fichiers *.txt présents dans le repertoire 'path'\n",
    "def all_txt_files_in_directory(path: str):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# infique si la ligne 'ligne' est une ligne valide ou si elle doit être ignorée (par exemple si elle vide)\n",
    "def is_valid_line(line: str) -> bool:\n",
    "    if line.startswith('Scène ') or line.startswith('Acte '):\n",
    "        return False\n",
    "    if len(line)<10:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split_book_into_paragraphs(path: str, min_paragraph_length) -> List[str]:\n",
    "    result = []\n",
    "    with open(path) as file:\n",
    "        current_paragraph = \"\"\n",
    "        for line in file:\n",
    "            if is_valid_line(line):\n",
    "                if current_paragraph :\n",
    "                    current_paragraph += \"\\n\"\n",
    "                current_paragraph += line.rstrip()\n",
    "                if len(current_paragraph) >= min_paragraph_length:\n",
    "                    result.append(current_paragraph) \n",
    "                    current_paragraph = \"\"\n",
    "    if len(current_paragraph) >= min_paragraph_length or (current_paragraph and len(result) == 0):\n",
    "        result.append(current_paragraph) \n",
    "    return result\n",
    "\n",
    "def load_all_books(path: str, min_paragraph_length:int) -> Dict[str,List[str]]:\n",
    "    book_to_paragraphs = dict()\n",
    "    for book_path in all_txt_files_in_directory(path):\n",
    "        book_to_paragraphs[pathlib.Path(book_path).stem] = split_book_into_paragraphs(book_path, min_paragraph_length)\n",
    "    return book_to_paragraphs\n",
    "\n",
    "\n",
    "\n",
    "class MaxHeap:\n",
    "    def __init__(self):\n",
    "        self.item_counts = dict()\n",
    "        self.total_count = 0\n",
    "        self.max_heap = []\n",
    "    def __str__(self):\n",
    "        most_common_original_word, count = self.get_max_item()\n",
    "        return f'{most_common_original_word} : {count}'\n",
    "    def add_item(self, original_word):\n",
    "        if original_word in self.item_counts:\n",
    "            self.item_counts[original_word] += 1\n",
    "        else:\n",
    "            self.item_counts[original_word] = 1\n",
    "        self.total_count += 1\n",
    "        heapq.heappush(self.max_heap, (-self.item_counts[original_word], original_word))\n",
    "        while self.max_heap and -self.max_heap[0][0] != self.item_counts[self.max_heap[0][1]]:\n",
    "            heapq.heappop(self.max_heap)\n",
    "    def most_common_original_word(self) -> str:\n",
    "        return self.get_max_item()[0]\n",
    "    def get_max_item(self):\n",
    "        if self.max_heap:\n",
    "            max_count, max_item = self.max_heap[0]\n",
    "            return max_item, -max_count\n",
    "        return None, 0\n",
    "\n",
    "\n",
    "# pour supprimer les accents\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def remove_diacritics(word: str) -> str:\n",
    "    import unidecode  \n",
    "    return unidecode.unidecode(word)\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def to_lowercase(word: str) -> str:\n",
    "    return word.lower()\n",
    "\n",
    "# pour le stemming\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def to_stemming(word: str) -> str:\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def normalize_word(word: str, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> str:\n",
    "    if use_lowercase:\n",
    "        word = to_lowercase(word)\n",
    "    if use_diacritics:\n",
    "        word = remove_diacritics(word)\n",
    "    if use_stemming:\n",
    "        word = to_stemming(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "   \n",
    "# return the list of stopwords found in author1 & author2\n",
    "def find_stopwords(normalized_words_to_stats_author1: dict, normalized_words_to_stats_author2: dict, hyperparameters: dict) -> Set[str]:\n",
    "        stopwords = set()\n",
    "        if not hyperparameters['use_stopword']:\n",
    "            return stopwords\n",
    "        stopword_min_frequency_in_percentage = hyperparameters['stopword_min_frequency_in_percentage'] \n",
    "        stopword_max_frequency_diff_in_percentage_between_authors = hyperparameters['stopword_max_frequency_diff_in_percentage_between_authors'] \n",
    "        stopword_max_length = hyperparameters['stopword_max_length'] \n",
    "        stopword_for_length_less_or_equal = hyperparameters['stopword_for_length_less_or_equal'] \n",
    "        \n",
    "        if stopword_max_length<=0:\n",
    "            return stopwords\n",
    "\n",
    "        total_words_author1 = sum([c.total_count for c in normalized_words_to_stats_author1.values()])\n",
    "        total_words_author2 = sum([c.total_count for c in normalized_words_to_stats_author2.values()])\n",
    "        for normalized_word, stats in normalized_words_to_stats_author1.items():\n",
    "            if normalized_word not in normalized_words_to_stats_author2:\n",
    "                continue\n",
    "                \n",
    "            if len(stats.most_common_original_word()) <= stopword_for_length_less_or_equal:\n",
    "                stopwords.add(normalized_word)\n",
    "                continue\n",
    "                \n",
    "            if len(stats.most_common_original_word())>stopword_max_length:\n",
    "                continue\n",
    "            #if len(stats.most_common_original_word())<=3:\n",
    "            #    stopwords.add(normalized_word)\n",
    "            #    continue\n",
    "            normalized_word_frequency_author1 = stats.total_count/total_words_author1\n",
    "            normalized_word_frequency_author2 = normalized_words_to_stats_author2[normalized_word].total_count/total_words_author2\n",
    "            if max(normalized_word_frequency_author1,normalized_word_frequency_author2)<stopword_min_frequency_in_percentage:\n",
    "                continue\n",
    "            frequency_diff_in_percentage_between_authors = 1-min(normalized_word_frequency_author1,normalized_word_frequency_author2)/max(normalized_word_frequency_author1,normalized_word_frequency_author2)\n",
    "            if frequency_diff_in_percentage_between_authors>stopword_max_frequency_diff_in_percentage_between_authors:\n",
    "                continue\n",
    "            stopwords.add(normalized_word)\n",
    "        return stopwords\n",
    "\n",
    "\n",
    "def paragraph_count(book_to_paragraphs: Dict[str, List[str]]) -> int:\n",
    "    if not book_to_paragraphs:\n",
    "        return 0\n",
    "    return sum([len(c) for c in book_to_paragraphs.values()])\n",
    "\n",
    "def split_text(text:str) -> List[str]:\n",
    "    return re.findall(r\"\\b[\\w'^\\d]+\\b\", text.rstrip())\n",
    "            \n",
    "def word_count(book_to_paragraphs: Dict[str, List[str]]) -> int:\n",
    "    result = 0\n",
    "    for paragraph in all_paragraphs(book_to_paragraphs):\n",
    "        result += len(split_text(paragraph.rstrip()))\n",
    "    return result\n",
    "\n",
    "def all_paragraphs(book_to_paragraphs: Dict[str, List[str]]) -> List[str]:\n",
    "    result = []\n",
    "    for p in book_to_paragraphs.values():\n",
    "        result.extend(p)\n",
    "    return result\n",
    "\n",
    "    \n",
    "\n",
    "# reduce the dataset so that it contains exactly 'target_count' paragraphs\n",
    "def reduce_to_paragraph_count(book_to_paragraphs: dict, target_count: int ) -> int:\n",
    "    current_count = paragraph_count(book_to_paragraphs)\n",
    "    if current_count<target_count:\n",
    "        raise Exception(f'current_count {current_count} < target_count {target_count}')\n",
    "    to_remove = current_count-  target_count\n",
    "    result = dict()\n",
    "    for book, paragraphs in sorted(book_to_paragraphs.items(), key =lambda x : len(x[1])):\n",
    "        if len(paragraphs)<=to_remove:\n",
    "            to_remove-=len(paragraphs)\n",
    "            continue\n",
    "        result[book] = paragraphs[:len(paragraphs)-to_remove]\n",
    "        to_remove = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_train_validation_single_author(book_to_paragraphs: dict, percentage_in_train:float) :\n",
    "    books = list(book_to_paragraphs.keys())\n",
    "    train = dict()\n",
    "    validation = dict()\n",
    "    for book, paragraphs in book_to_paragraphs.items():\n",
    "        paragraphs_count = len(paragraphs)\n",
    "        percentage_in_train_if_adding_to_train = (paragraph_count(train)+len(paragraphs))/max(paragraph_count(train)+paragraph_count(validation)+len(paragraphs),1)\n",
    "        percentage_in_train_if_adding_to_validation = paragraph_count(train)/max(paragraph_count(train)+paragraph_count(validation)+len(paragraphs),1)\n",
    "        if abs(percentage_in_train_if_adding_to_train-percentage_in_train)<abs(percentage_in_train_if_adding_to_validation-percentage_in_train):\n",
    "            train[book] = paragraphs\n",
    "        else:\n",
    "            validation[book] = paragraphs\n",
    "    return train, validation\n",
    "    \n",
    "def split_train_validation_all_authors(book_to_paragraphs_author1: dict, book_to_paragraphs_author2: dict, percentage_in_train:float) :\n",
    "    train_author1,validation_author1 = split_train_validation_single_author(book_to_paragraphs_author1, percentage_in_train)\n",
    "    train_author2,validation_author2 = split_train_validation_single_author(book_to_paragraphs_author2, percentage_in_train)\n",
    "\n",
    "    target_length_validation = min(paragraph_count(validation_author1),paragraph_count(validation_author2))            \n",
    "    validation_author1 = reduce_to_paragraph_count(validation_author1, target_length_validation)\n",
    "    validation_author2 = reduce_to_paragraph_count(validation_author2, target_length_validation)\n",
    "\n",
    "    target_length_train = min(paragraph_count(train_author1),paragraph_count(train_author2))            \n",
    "    train_author1 = reduce_to_paragraph_count(train_author1, target_length_train)\n",
    "    train_author2 = reduce_to_paragraph_count(train_author2, target_length_train)\n",
    "\n",
    "    proportion_in_train = target_length_train/(target_length_train+target_length_validation)\n",
    "    if proportion_in_train>percentage_in_train:\n",
    "        target_length_train = int( (percentage_in_train/(1-percentage_in_train)) *target_length_validation )\n",
    "        train_author1 = reduce_to_paragraph_count(train_author1, target_length_train)\n",
    "        train_author2 = reduce_to_paragraph_count(train_author2, target_length_train)\n",
    "    else:\n",
    "        target_length_validation = int( ((1-percentage_in_train)/percentage_in_train) *target_length_train )\n",
    "        validation_author1 = reduce_to_paragraph_count(validation_author1, target_length_validation)\n",
    "        validation_author2 = reduce_to_paragraph_count(validation_author2, target_length_validation)\n",
    "    return train_author1,validation_author1,train_author2,validation_author2\n",
    "\n",
    "\n",
    "def compute_normalized_words_to_stats(paragraphs: List[str], hyperparameters: dict) -> dict:\n",
    "    use_lowercase = hyperparameters['use_lowercase']\n",
    "    use_diacritics = hyperparameters['use_diacritics']\n",
    "    use_stemming = hyperparameters['use_stemming']\n",
    "    normalized_words_to_stats = dict()\n",
    "    for paragraph in paragraphs:\n",
    "        words = re.findall(r\"\\b[\\w'^\\d]+\\b\", paragraph.rstrip())\n",
    "        for original_word in words:\n",
    "            normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "            if normalized_word not in normalized_words_to_stats:\n",
    "                normalized_words_to_stats[normalized_word] = MaxHeap()\n",
    "            normalized_words_to_stats[normalized_word].add_item(original_word)    \n",
    "    return normalized_words_to_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3baff9",
   "metadata": {},
   "source": [
    "## Création d'un fichier de statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd281715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 150\n",
    "hyperparameters['use_lowercase'] = True\n",
    "hyperparameters['use_diacritics'] = True\n",
    "hyperparameters['use_stemming'] = True\n",
    "\n",
    "moliere_dataset = load_all_books(os.path.join(directory, 'moliere'), hyperparameters['min_paragraph_length'])\n",
    "corneille_dataset = load_all_books(os.path.join(directory, 'corneille'), hyperparameters['min_paragraph_length'])\n",
    "\n",
    "\n",
    "stats_moliere = compute_normalized_words_to_stats(all_paragraphs(moliere_dataset), hyperparameters)\n",
    "stats_corneille = compute_normalized_words_to_stats(all_paragraphs(corneille_dataset), hyperparameters)\n",
    "\n",
    "moliere_total_word_count = sum([c.total_count for c in stats_moliere.values()])\n",
    "corneille_total_word_count = sum([c.total_count for c in stats_corneille.values()])\n",
    "\n",
    "normalized_words = list((set(stats_moliere.keys())|set(stats_corneille.keys())))\n",
    "normalized_words.sort()\n",
    "moliere_normalized_word_count_in_percentage = []\n",
    "moliere_normalized_word_count = []\n",
    "corneille_normalized_word_count_in_percentage = []\n",
    "corneille_normalized_word_count = []\n",
    "moliere_most_common_original_word = []\n",
    "moliere_most_common_original_word_count = []\n",
    "corneille_most_common_original_word = []\n",
    "corneille_most_common_original_word_count = []\n",
    "\n",
    "\n",
    "for normalized_word in normalized_words:\n",
    "    if normalized_word in stats_moliere:\n",
    "        stat = stats_moliere[normalized_word]\n",
    "        moliere_normalized_word_count_in_percentage.append(stat.total_count/moliere_total_word_count)\n",
    "        moliere_normalized_word_count.append(stat.total_count)\n",
    "        most_common_word, most_common_word_count = stat.get_max_item()\n",
    "        moliere_most_common_original_word.append(most_common_word)\n",
    "        moliere_most_common_original_word_count.append(most_common_word_count)\n",
    "    else:\n",
    "        moliere_normalized_word_count_in_percentage.append(0)\n",
    "        moliere_normalized_word_count.append(0)\n",
    "        moliere_most_common_original_word.append(None)\n",
    "        moliere_most_common_original_word_count.append(None)\n",
    "    if normalized_word in stats_corneille:\n",
    "        stat = stats_corneille[normalized_word]\n",
    "        corneille_normalized_word_count_in_percentage.append(stat.total_count/corneille_total_word_count)\n",
    "        corneille_normalized_word_count.append(stat.total_count)\n",
    "        most_common_word, most_common_word_count = stat.get_max_item()\n",
    "        corneille_most_common_original_word.append(most_common_word)\n",
    "        corneille_most_common_original_word_count.append(most_common_word_count)\n",
    "    else:\n",
    "        corneille_normalized_word_count_in_percentage.append(0)\n",
    "        corneille_normalized_word_count.append(0)\n",
    "        corneille_most_common_original_word.append(None)\n",
    "        corneille_most_common_original_word_count.append(None)\n",
    "\n",
    "fhr_stats = pd.DataFrame(\n",
    "    {'normalized_words': normalized_words,\n",
    "    'moliere_count_in_percentage': moliere_normalized_word_count_in_percentage,\n",
    "    'moliere_count': moliere_normalized_word_count,\n",
    "    'corneille_count_in_percentage' : corneille_normalized_word_count_in_percentage,\n",
    "    'corneille_count' : corneille_normalized_word_count,\n",
    "    'moliere_most_common_original_word' : moliere_most_common_original_word,\n",
    "    'moliere_most_common_original_word_count' : moliere_most_common_original_word_count,\n",
    "    'corneille_most_common_original_word' : corneille_most_common_original_word,\n",
    "    'corneille_most_common_original_word_count' : corneille_most_common_original_word_count,\n",
    "    })\n",
    "\n",
    "# on sauvegarde ces stats sur le disque\n",
    "fhr_stats.to_csv(os.path.join(directory, 'stylometrie_stats.csv'), index=False)         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5078a11",
   "metadata": {},
   "source": [
    "## Entraînement et calcul des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcul_f1_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (2*TP)/max(TP+FP+FN,1)\n",
    "def calcul_accuracy_target_0(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TN/max(TN+FP,1)\n",
    "def calcul_accuracy_target_1(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TP/max(TP+FN,1)\n",
    "def calcul_accuracy(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (TP+TN)/max(TP+TN+FP+FN,1)\n",
    "\n",
    "\n",
    "def delete_keys(data:dict, keys_to_delete) -> None:\n",
    "    for c in keys_to_delete:\n",
    "        if c in data:\n",
    "            del data[c]\n",
    "\n",
    "def compute_author_score_v1(text:str, most_common_words_for_author: dict, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "def compute_author_score_v2(text:str, most_common_words_for_author: dict, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author:\n",
    "            score += 1+most_common_words_for_author[normalized_word]\n",
    "    return score\n",
    "\n",
    "def compute_author_score_v3(text:str, most_common_words_for_author: dict, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    used_words = set()\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author and normalized_word not in used_words:\n",
    "            score += 1\n",
    "            used_words.add(normalized_word)\n",
    "    return score\n",
    "\n",
    "def compute_author_score_v4(text:str, most_common_words_for_author: dict, use_lowercase: bool, use_diacritics: bool, use_stemming: bool) -> float:\n",
    "    used_words = set()\n",
    "    score = 0\n",
    "    for original_word in split_text(text):\n",
    "        normalized_word = normalize_word(original_word, use_lowercase, use_diacritics, use_stemming)\n",
    "        if normalized_word in most_common_words_for_author and normalized_word not in used_words:\n",
    "            score += 1+most_common_words_for_author[normalized_word]\n",
    "            used_words.add(normalized_word)\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(text_author1: List[str], text_author2: List[str], most_common_words_author1: dict , most_common_words_author2: dict, hyperparameters: dict, verbose:bool) ->Tuple[int,int,int,int]:\n",
    "    TP = 0 # y_true = Molière ,  y_pred = Molière\n",
    "    TN = 0 # y_true = Corneille, y_pred = Corneille\n",
    "    FN = 0 # y_true = Molière,   y_pred = Corneille\n",
    "    FP = 0 # y_true = Corneille, y_pred = Molière \n",
    "    compute_author_score = hyperparameters['compute_author_score']\n",
    "    use_lowercase = hyperparameters['use_lowercase']\n",
    "    use_diacritics = hyperparameters['use_diacritics']\n",
    "    use_stemming = hyperparameters['use_stemming']\n",
    "    equalities = 0\n",
    "    for t in text_author1:\n",
    "        score_author1 = compute_author_score(t, most_common_words_author1, use_lowercase, use_diacritics, use_stemming)\n",
    "        score_author2 = compute_author_score(t, most_common_words_author2, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author1 == score_author2: equalities +=1\n",
    "        if score_author1>score_author2 or (score_author1==score_author2 and random.random() > 0.5):\n",
    "            if TP == 0 and verbose:\n",
    "                print(f'\\nExemple de TP (Texte de Molière, bien identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            TP += 1\n",
    "        else:\n",
    "            if FN == 0 and verbose:\n",
    "                print(f'\\nExemple de FN (Texte de Molière, mal identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            FN += 1\n",
    "    for t in text_author2:\n",
    "        score_author1 = compute_author_score(t, most_common_words_author1, use_lowercase, use_diacritics, use_stemming)\n",
    "        score_author2 = compute_author_score(t, most_common_words_author2, use_lowercase, use_diacritics, use_stemming)\n",
    "        if score_author1 == score_author2: equalities +=1\n",
    "        if score_author1>score_author2 or (score_author1==score_author2 and random.random() > 0.5):\n",
    "            if FP == 0 and verbose:\n",
    "                print(f'\\nExemple de FP (Texte de Corneille, mal identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            FP += 1\n",
    "        else:\n",
    "            if TN == 0 and verbose:\n",
    "                print(f'\\nExemple de TN (Texte de Corneille, bien identifié, score Molière: {round(score_author1,4)}, score Corneille: {round(score_author2,4)}):\\n{t}\\n')\n",
    "            TN += 1\n",
    "    if verbose:\n",
    "        print(f'Non différentiables: {equalities/max(len(text_author1)+len(text_author2),1)}')\n",
    "    return (TP,TN,FP,FN)\n",
    "        \n",
    "    \n",
    "def train(hyperparameters: dict, verbose: bool):\n",
    "    random.seed(42)\n",
    "    min_paragraph_length = hyperparameters['min_paragraph_length']\n",
    "    moliere_dataset = load_all_books(os.path.join(directory, 'moliere'), min_paragraph_length)\n",
    "    corneille_dataset = load_all_books(os.path.join(directory, 'corneille'), min_paragraph_length)\n",
    "    if verbose: \n",
    "        print(f'\\nMoliere Dataset: {paragraph_count(moliere_dataset)} paragraphes ({word_count(moliere_dataset)} mots) venant de {len(moliere_dataset)} oeuvres:\\n{list(moliere_dataset.keys())}')\n",
    "        print(f'\\nCorneille Dataset: {paragraph_count(corneille_dataset)} paragraphes ({word_count(corneille_dataset)} mots) venant de {len(corneille_dataset)} oeuvres:\\n{list(corneille_dataset.keys())}')\n",
    "\n",
    "    percentage_in_train = hyperparameters['percentage_in_train']\n",
    "    train_moliere,validation_moliere,train_corneille,validation_corneille = split_train_validation_all_authors(moliere_dataset, corneille_dataset, percentage_in_train)\n",
    "\n",
    "    if verbose: \n",
    "        print(f'\\nMoliere Train Dataset: {paragraph_count(train_moliere)} paragraphes ({word_count(train_moliere)} mots) venant de {len(train_moliere)} oeuvres:\\n{list(train_moliere.keys())}')\n",
    "        print(f'\\nMoliere Validation Dataset: {paragraph_count(validation_moliere)} paragraphes ({word_count(validation_moliere)} mots) venant de {len(validation_moliere)} oeuvres:\\n{list(validation_moliere.keys())}')\n",
    "        print(f'\\nCorneille Train Dataset: {paragraph_count(train_corneille)} paragraphes ({word_count(train_corneille)} mots) venant de {len(train_corneille)} oeuvres:\\n{list(train_corneille.keys())}')\n",
    "        print(f'\\nCorneille Validation Dataset: {paragraph_count(validation_corneille)} paragraphes ({word_count(validation_corneille)} mots) venant de {len(validation_corneille)} oeuvres:\\n{list(validation_corneille.keys())}')\n",
    "\n",
    "    normalized_words_to_stats_moliere = compute_normalized_words_to_stats(all_paragraphs(train_moliere), hyperparameters)\n",
    "    normalized_words_to_stats_corneille = compute_normalized_words_to_stats(all_paragraphs(train_corneille), hyperparameters)\n",
    "\n",
    "    if hyperparameters['use_stopword']:\n",
    "        stopwords = find_stopwords(normalized_words_to_stats_moliere, normalized_words_to_stats_corneille, hyperparameters)\n",
    "        print(f'{len(stopwords)} mots vides : {list(stopwords)[:10]}')\n",
    "        # we remove stopwords\n",
    "        delete_keys(normalized_words_to_stats_moliere, stopwords)\n",
    "        delete_keys(normalized_words_to_stats_corneille, stopwords)\n",
    "\n",
    "    def most_common_normalized_words(normalized_words_to_stats: dict, count: int) -> Dict[str,float]:\n",
    "        sorted_by_total_count = sorted(normalized_words_to_stats.items(), key=lambda item:item[1].total_count, reverse=True)[:count]\n",
    "        total_count = sum([c[1].total_count for c in sorted_by_total_count])\n",
    "        result  = dict()\n",
    "        for normalied_word,stats in sorted_by_total_count:\n",
    "            result[normalied_word] = stats.total_count/total_count\n",
    "        return result\n",
    "\n",
    "\n",
    "    most_common_normalized_words_count = hyperparameters['most_common_normalized_words_count']\n",
    "\n",
    "    # we only keep the most common words\n",
    "    most_common_moliere = most_common_normalized_words(normalized_words_to_stats_moliere, most_common_normalized_words_count)\n",
    "    most_common_corneille = most_common_normalized_words(normalized_words_to_stats_corneille, most_common_normalized_words_count)\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix(all_paragraphs(validation_moliere), all_paragraphs(validation_corneille), most_common_moliere , most_common_corneille, hyperparameters, verbose)\n",
    "    return (TP,TN,FP,FN,train_moliere,validation_moliere,train_corneille,validation_corneille,normalized_words_to_stats_moliere, normalized_words_to_stats_corneille, most_common_moliere, most_common_corneille)\n",
    "\n",
    "verbose = False\n",
    "\n",
    "hyperparameters = dict()\n",
    "hyperparameters['min_paragraph_length'] = 150\n",
    "hyperparameters['percentage_in_train'] = 0.8\n",
    "hyperparameters['most_common_normalized_words_count'] = 300\n",
    "hyperparameters['use_lowercase'] = True\n",
    "hyperparameters['use_diacritics'] = True\n",
    "hyperparameters['use_stemming'] = True\n",
    "hyperparameters['compute_author_score'] = compute_author_score_v1\n",
    "hyperparameters['use_stopword'] = False\n",
    "#hyperparameters['stopword_min_frequency_in_percentage'] = 0.001\n",
    "#hyperparameters['stopword_max_frequency_diff_in_percentage_between_authors'] = 0.2\n",
    "#hyperparameters['stopword_max_length'] = 2\n",
    "#hyperparameters['stopword_for_length_less_or_equal'] = 0\n",
    "\n",
    "#if verbose:    print(hyperparameters)\n",
    "(TP,TN,FP,FN,train_moliere,validation_moliere,train_corneille,validation_corneille,normalized_words_to_stats_moliere, normalized_words_to_stats_corneille, most_common_moliere, most_common_corneille) = train(hyperparameters, verbose)\n",
    "print()\n",
    "print('-'*80)\n",
    "print(f'Validation Accuracy: {round(calcul_accuracy(TP,TN,FP,FN),4)}  (Accuracy(Moliere): {round(calcul_accuracy_target_1(TP,TN,FP,FN),4)} / Accuracy(Corneille): {round(calcul_accuracy_target_0(TP,TN,FP,FN),4)})')\n",
    "print('-'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433c120",
   "metadata": {},
   "source": [
    "## Affichage des données sous la forme d'un nuage de mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wordcloud(most_common_normalized_words: dict, normalized_words_to_stats_moliere: dict, title:str) -> None:\n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "    word_frequencies = dict()\n",
    "    for normalized_word, frequency in most_common_normalized_words.items():\n",
    "        original_word = normalized_words_to_stats_moliere[normalized_word].most_common_original_word()\n",
    "        if len(original_word) > 4:\n",
    "            word_frequencies[original_word] = frequency\n",
    "    wordcloud = WordCloud(width=1200, height=600, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "    # Display the generated word cloud\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "display_wordcloud(most_common_moliere, normalized_words_to_stats_moliere, \"Moliere (uniquement les mots d'au moins 5 lettres)\")\n",
    "display_wordcloud(most_common_corneille, normalized_words_to_stats_corneille, \"Corneille (uniquement les mots d'au moins 5 lettres)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84132b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mots (d'au moins 5 lettres) les plus fréquents chez Moliere\\n\", \"\\n\".join([str(c) for c in most_common_moliere.items() if len(c[0])>4][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mots (d'au moins 5 lettres) les plus fréquents chez Corneille\\n\", \"\\n\".join([str(c) for c in most_common_corneille.items() if len(c[0])>5][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for normalized_word in ['monsieur', 'madam', 'plus', 'esprit', 'encor']:\n",
    "    print(f\"Le mot '{normalized_word}':\")\n",
    "    print(f'\\test présent {normalized_words_to_stats_moliere[normalized_word].total_count} fois chez Moliere:  ', normalized_words_to_stats_moliere[normalized_word].item_counts)\n",
    "    print(f'\\test présent {normalized_words_to_stats_corneille[normalized_word].total_count} fois chez Corneille:', normalized_words_to_stats_corneille[normalized_word].item_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fef9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_moliere['les_fourberies_de_scapin'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721f0fd",
   "metadata": {},
   "source": [
    "## Précision pour chaque oeuvre utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*80+'\\nPrécision pour chaque oeuvre de Moliere\\n'+'-'*80)\n",
    "accuracy_moliere = dict()\n",
    "for book_path in all_txt_files_in_directory(os.path.join(directory, 'moliere')):\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix(split_book_into_paragraphs(book_path, hyperparameters['min_paragraph_length']), [], most_common_moliere , most_common_corneille, hyperparameters, False)\n",
    "    #print(f\"Accuracy '{pathlib.Path(book_path).stem}': {round(calcul_accuracy(TP,TN,FP,FN),4)}\")\n",
    "    accuracy_moliere[pathlib.Path(book_path).stem] = calcul_accuracy(TP,TN,FP,FN)\n",
    "for e in sorted(accuracy_moliere.items(), key=lambda x: x[1]): print(e)\n",
    "\n",
    "\n",
    "    \n",
    "print()\n",
    "print('-'*80+'\\nPrécision pour chaque oeuvre de Corneille\\n'+'-'*80)\n",
    "accuracy_corneille = dict()\n",
    "for book_path in all_txt_files_in_directory(os.path.join(directory, 'corneille')):\n",
    "    (TP,TN,FP,FN) = compute_confusion_matrix([], split_book_into_paragraphs(book_path, hyperparameters['min_paragraph_length']), most_common_moliere , most_common_corneille, hyperparameters, False)\n",
    "    accuracy_corneille[pathlib.Path(book_path).stem] = calcul_accuracy(TP,TN,FP,FN)\n",
    "for e in sorted(accuracy_corneille.items(), key=lambda x: x[1]): print(e)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
